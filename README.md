# ğŸš¢ SystÃ¨me de Suivi Maritime en Temps RÃ©el - Projet Big Data

## ğŸ“‹ Description du Projet

**Maritime Tracking System** est un systÃ¨me complet de suivi en temps rÃ©el d'une flotte maritime mÃ©diterranÃ©enne utilisant un pipeline Big Data distribuÃ© et scalable. Ce projet dÃ©montre l'intÃ©gration de technologies modernes (Kafka, Spark, Hadoop, Impala) pour l'ingestion, le traitement en continu et l'analyse de donnÃ©es IoT provenant de navires commerciaux.

Ce systÃ¨me simule une flotte de navires en transit entre diffÃ©rents ports mÃ©diterranÃ©ens, gÃ©nÃ¨re des donnÃ©es de tÃ©lÃ©mÃ©trie en continu, les traite en temps rÃ©el, les stocke de maniÃ¨re distribuÃ©e et fournit des analyses approfondies pour l'optimisation des opÃ©rations maritimes.

---

## ğŸ¯ Cas d'Usage et Objectifs

### Cas d'Usage Principal

Suivi de navires commerciaux (porte-conteneurs, pÃ©troliers, vraquiers) sur des routes maritimes mÃ©diterranÃ©ennes avec monitoring en temps rÃ©el et analyses prÃ©dictives.

### DonnÃ©es CollectÃ©es

Chaque navire envoie des donnÃ©es de tÃ©lÃ©mÃ©trie incluant :

- **Position GPS** : Latitude, longitude, horodatage
- **Navigation** : Vitesse (nÅ“uds), cap (0-360Â°), profondeur de l'eau
- **Propulsion** : Consommation carburant (litres), tempÃ©rature moteur, RPM
- **Cargo** : Poids transportÃ©, occupation des conteneurs
- **Conditions** : MÃ©tÃ©o, hauteur des vagues, direction du vent

### Objectifs MÃ©tier

1. **Monitoring en Temps RÃ©el** : Suivi instantanÃ© de la position et de l'Ã©tat de chaque navire
2. **Alertes Automatiques** : Carburant bas, anomalies moteur, vitesse anormale, dÃ©gradation mÃ©tÃ©o
3. **Analyses PrÃ©dictives** : ETA (Estimated Time of Arrival), maintenance prÃ©dictive, score de risque
4. **Optimisation de Routes** : Analyse des performances par trajectoire, recommandations de routes optimales
5. **Reporting et ConformitÃ©** : DonnÃ©es historiques pour audits et conformitÃ© rÃ©glementaire

---

## ğŸ—ï¸ Architecture du Pipeline

### Flux GÃ©nÃ©ral des DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MARITIME TRACKING SYSTEM                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1 : GÃ‰NÃ‰RATION DES DONNÃ‰ES (Scala)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  DataGenerator.scala                                                    â”‚
â”‚  â”œâ”€ Simule N navires                                                    â”‚
â”‚  â”œâ”€ GÃ©nÃ¨re positions GPS basÃ©es sur formule Haversine                   â”‚
â”‚  â”œâ”€ Calcule vitesse, cap, consommation carburant                        â”‚
â”‚  â”œâ”€ GÃ©nÃ¨re anomalies alÃ©atoires (carburant bas, moteur chaud)           â”‚
â”‚  â””â”€ Format: JSON pour streaming                                         â”‚
â”‚                                                                          â”‚
â”‚  Navires simulÃ©s:                                                       â”‚
â”‚  â€¢ SHIP_001: Route Tanger â†’ Marseille (850 nm) - Port type: Conteneurs â”‚
â”‚  â€¢ SHIP_002: Route Barcelona â†’ Alger (320 nm)   - Port type: PÃ©trolier â”‚
â”‚  â€¢ SHIP_003: Route AthÃ¨nes â†’ Naples (550 nm)    - Port type: Vraquiers â”‚
â”‚  â€¢ ... (jusqu'Ã  20+ navires configurables)                             â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ JSON Telemetry (ShipTelemetry case class)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 2 : INGESTION TEMPS RÃ‰EL (Kafka Producer)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  KafkaProducer.scala                                                    â”‚
â”‚  â”œâ”€ Brokers: localhost:9092                                             â”‚
â”‚  â”œâ”€ Topics:                                                             â”‚
â”‚  â”‚  â”œâ”€ maritime-tracking (8 partitions)  : Tous les points de donnÃ©es   â”‚
â”‚  â”‚  â””â”€ maritime-alerts (4 partitions)    : Uniquement anomalies         â”‚
â”‚  â”œâ”€ Partitionnement: Par navire_id (key) pour ordre garanti             â”‚
â”‚  â”œâ”€ DÃ©bit: ~100-500 messages/seconde selon config                      â”‚
â”‚  â””â”€ SÃ©rialisation: JSON                                                 â”‚
â”‚                                                                          â”‚
â”‚  Exemple message:                                                       â”‚
â”‚  {                                                                       â”‚
â”‚    "navire_id": "SHIP_001",                                             â”‚
â”‚    "timestamp": "2025-12-25T14:32:45Z",                                 â”‚
â”‚    "latitude": 43.2965, "longitude": 5.3698,                            â”‚
â”‚    "vitesse_noeuds": 12.5, "cap_degres": 180.0,                         â”‚
â”‚    "carburant_litres": 45000,                                           â”‚
â”‚    "temperature_moteur_celsius": 78.5,                                  â”‚
â”‚    "anomalies": ["CARBURANT_BAS"]                                       â”‚
â”‚  }                                                                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Streaming Kafka Topic
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 3 : TRAITEMENT EN TEMPS RÃ‰EL (Spark Streaming)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  SparkStreaming.scala                                                   â”‚
â”‚  â”œâ”€ Micro-batch interval: 2 secondes                                    â”‚
â”‚  â”œâ”€ Watermarking: TolÃ©rance 10 minutes pour donnÃ©es tardives            â”‚
â”‚  â”‚                                                                       â”‚
â”‚  â”œâ”€ Transformations:                                                    â”‚
â”‚  â”‚  â”œâ”€ Parsing JSON â†’ DataFrames                                        â”‚
â”‚  â”‚  â”œâ”€ DÃ©tection anomalies:                                             â”‚
â”‚  â”‚  â”‚   â€¢ Carburant < 20% de capacitÃ©                                   â”‚
â”‚  â”‚  â”‚   â€¢ TempÃ©rature moteur > 85Â°C                                     â”‚
â”‚  â”‚  â”‚   â€¢ Vitesse anomale (< 2 ou > 25 nÅ“uds)                          â”‚
â”‚  â”‚  â”œâ”€ AgrÃ©gations fenÃªtrÃ©es (5 minutes):                               â”‚
â”‚  â”‚  â”‚   â€¢ Vitesse moyenne par navire                                    â”‚
â”‚  â”‚  â”‚   â€¢ Consommation carburant cumulative                             â”‚
â”‚  â”‚  â”‚   â€¢ Distance parcourue (Haversine)                                â”‚
â”‚  â”‚  â”‚   â€¢ ETA computation                                               â”‚
â”‚  â”‚  â””â”€ Calculs additionnels:                                            â”‚
â”‚  â”‚      â€¢ EfficacitÃ© Ã©nergÃ©tique (nm par litre)                         â”‚
â”‚  â”‚      â€¢ Score d'alerte global                                         â”‚
â”‚  â”‚                                                                       â”‚
â”‚  â””â”€ Output: Ã‰criture partitionnÃ©e en Parquet                            â”‚
â”‚                                                                          â”‚
â”‚  3 flux de sortie:                                                      â”‚
â”‚  1. Maritime Raw Data    : Chaque point de donnÃ©es                      â”‚
â”‚  2. Maritime Aggregated  : AgrÃ©gations par fenÃªtre                      â”‚
â”‚  3. Maritime Anomalies   : Alertes et exceptions                        â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Parquet Files
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 4 : STOCKAGE DISTRIBUÃ‰ (HDFS - Hadoop)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  NameNode: hdfs://namenode:9000                                         â”‚
â”‚  Replication Factor: 3 (tolÃ©rance 2 nÅ“uds)                              â”‚
â”‚                                                                          â”‚
â”‚  HiÃ©rarchie HDFS:                                                       â”‚
â”‚  /maritime/                                                             â”‚
â”‚  â”œâ”€ raw_data/                 : DonnÃ©es brutes temps rÃ©el               â”‚
â”‚  â”‚  â”œâ”€ date=2025-12-25/                                                â”‚
â”‚  â”‚  â”‚  â”œâ”€ hour=14/                                                     â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€ part-00000*.parquet                                        â”‚
â”‚  â”‚  â”‚  â””â”€ hour=15/                                                     â”‚
â”‚  â”‚  â””â”€ ...                                                             â”‚
â”‚  â”œâ”€ aggregated/               : AgrÃ©gations 5 min                       â”‚
â”‚  â”‚  â”œâ”€ date=2025-12-25/                                                â”‚
â”‚  â”‚  â”‚  â”œâ”€ navire_id=SHIP_001/                                           â”‚
â”‚  â”‚  â”‚  â””â”€ navire_id=SHIP_002/                                           â”‚
â”‚  â”‚  â””â”€ ...                                                             â”‚
â”‚  â”œâ”€ anomalies/                : Alertes et exceptions                   â”‚
â”‚  â”‚  â””â”€ alertes-2025-12-25.parquet                                       â”‚
â”‚  â”œâ”€ eta_predictions/          : PrÃ©dictions d'arrivÃ©e                   â”‚
â”‚  â”‚  â””â”€ predictions-2025-12-25.parquet                                   â”‚
â”‚  â””â”€ analysis/                 : RÃ©sultats analyses batch                â”‚
â”‚     â”œâ”€ ship_statistics/       : Stats par navire                        â”‚
â”‚     â”œâ”€ route_performance/     : Performance par route                   â”‚
â”‚     â”œâ”€ temporal_analysis/     : Ã‰volution horaire                       â”‚
â”‚     â”œâ”€ weather_impact/        : Impact mÃ©tÃ©o                            â”‚
â”‚     â”œâ”€ anomalies_detected/    : Historique anomalies                    â”‚
â”‚     â””â”€ maintenance_prediction/: Score risque maintenance                â”‚
â”‚                                                                          â”‚
â”‚  Format: Parquet compressÃ© (Snappy)                                     â”‚
â”‚  Partitionnement: date, navire_id, heure                                â”‚
â”‚  Volume: ~1-5 GB par jour selon dÃ©bit                                   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Spark SQL Queries
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 5 : REQUÃŠTES SQL DISTRIBUÃ‰ES (Impala/Hive)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Tables Externes (sur donnÃ©es Parquet HDFS):                            â”‚
â”‚  â”œâ”€ maritime_raw_data        : Flux brut de tous points                 â”‚
â”‚  â”œâ”€ maritime_aggregated      : AgrÃ©gations par navire/fenÃªtre           â”‚
â”‚  â”œâ”€ maritime_anomalies       : Historique alertes                       â”‚
â”‚  â”œâ”€ maritime_eta             : PrÃ©dictions arrivÃ©e                      â”‚
â”‚  â””â”€ maritime_vessel_info     : Metadata navires                         â”‚
â”‚                                                                          â”‚
â”‚  Vues SQL (Materialized):                                               â”‚
â”‚  â”œâ”€ v_current_positions      : DerniÃ¨re position connue par navire      â”‚
â”‚  â”œâ”€ v_active_alerts          : Alertes actuelles non rÃ©solues           â”‚
â”‚  â”œâ”€ v_ships_requiring_attention: Navires avec anomalies                 â”‚
â”‚  â”œâ”€ v_route_efficiency       : EfficacitÃ© Ã©nergÃ©tique par route         â”‚
â”‚  â””â”€ v_maintenance_alerts     : Navires nÃ©cessitant maintenance          â”‚
â”‚                                                                          â”‚
â”‚  Performances:                                                          â”‚
â”‚  â€¢ RequÃªtes simples: < 500ms                                            â”‚
â”‚  â€¢ AgrÃ©gations complexes: < 5s                                          â”‚
â”‚  â€¢ Scan complet historique: < 30s                                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Pandas DataFrames
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 6 : VISUALISATION & REPORTING (Jupyter)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  maritime_analysis.ipynb                                                â”‚
â”‚  â”œâ”€ Graphiques temps rÃ©el:                                              â”‚
â”‚  â”‚  â”œâ”€ Cartes Folium des trajectoires                                   â”‚
â”‚  â”‚  â”œâ”€ Heatmaps des zones de trafic                                     â”‚
â”‚  â”‚  â”œâ”€ Cartes de chaleur consommation carburant                         â”‚
â”‚  â”‚  â””â”€ Timeline des anomalies dÃ©tectÃ©es                                 â”‚
â”‚  â”‚                                                                       â”‚
â”‚  â”œâ”€ Analyses statistiques:                                              â”‚
â”‚  â”‚  â”œâ”€ Histogrammes vitesse/consommation                                â”‚
â”‚  â”‚  â”œâ”€ CorrÃ©lations vitesse-consommation                                â”‚
â”‚  â”‚  â”œâ”€ Distribution des anomalies par type                              â”‚
â”‚  â”‚  â””â”€ Box plots comparaison navires                                    â”‚
â”‚  â”‚                                                                       â”‚
â”‚  â”œâ”€ Dashboards:                                                         â”‚
â”‚  â”‚  â”œâ”€ Vue synthÃ©tique flotte (status par navire)                       â”‚
â”‚  â”‚  â”œâ”€ Tableau bord opÃ©rations (alertes actives)                        â”‚
â”‚  â”‚  â”œâ”€ KPIs clÃ©s (total distance, carburant consommÃ©, etc)              â”‚
â”‚  â”‚  â””â”€ Tendances historiques                                            â”‚
â”‚  â”‚                                                                       â”‚
â”‚  â””â”€ Export rÃ©sultats: PDF, PNG, CSV                                     â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux Technologique SimplifiÃ©

```
DonnÃ©es Brutes â†’ Kafka â†’ Spark Streaming â†’ HDFS â†’ Impala/Spark SQL â†’ Visualisation
   (IoT)       (Ingestion) (Traitement)    (Stockage) (RequÃªtes)      (Insights)
```

---

## ğŸ› ï¸ Technologies UtilisÃ©es

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **Ingestion** | Kafka 3.5.0 | Message broker temps rÃ©el |
| **Traitement** | Spark 3.5.0 (Scala 2.12) | Streaming + Batch processing |
| **Stockage** | HDFS (Hadoop 3.2.1) | SystÃ¨me de fichiers distribuÃ© |
| **SQL** | Hive/Impala + Spark SQL | RequÃªtes sur donnÃ©es massives |
| **Orchestration** | Docker Compose | Gestion des conteneurs |
| **Visualisation** | Jupyter (PySpark) | Notebooks interactifs |

---


```
maritime-tracking/
â”œâ”€â”€ docker-compose.yml          # Configuration des services
â”œâ”€â”€ README.md                   # Ce fichier
â”‚
â”œâ”€â”€ scala-app/                  # Application Scala
â”‚   â”œâ”€â”€ build.sbt              # Configuration SBT
â”‚   â”œâ”€â”€ project/
â”‚   â”‚   â”œâ”€â”€ build.properties
â”‚   â”‚   â””â”€â”€ plugins.sbt
â”‚   â””â”€â”€ src/main/scala/maritime/
â”‚       â”œâ”€â”€ DataGenerator.scala      # Simulation navires
â”‚       â”œâ”€â”€ KafkaProducer.scala      # Producer Kafka
â”‚       â”œâ”€â”€ SparkStreaming.scala     # Traitement temps rÃ©el
â”‚       â””â”€â”€ SparkBatch.scala         # Analyses batch
â”‚
â”œâ”€â”€ scripts/                    # Scripts d'automatisation
â”‚   â”œâ”€â”€ setup-and-run.sh       # Installation complÃ¨te
â”‚   â”œâ”€â”€ run-producer.sh        # Lancer le producer
â”‚   â”œâ”€â”€ run-streaming.sh       # Lancer Spark Streaming
â”‚   â””â”€â”€ run-batch.sh           # Lancer analyses batch
â”‚
â”œâ”€â”€ sql/                        # RequÃªtes SQL
â”‚   â””â”€â”€ create-tables.sql      # CrÃ©ation tables Impala/Hive
â”‚
â”œâ”€â”€ notebooks/                  # Notebooks Jupyter
â”‚   â””â”€â”€ analysis.ipynb         # Analyses et visualisations
â”‚
â””â”€â”€ data/                       # DonnÃ©es de rÃ©fÃ©rence
    â””â”€â”€ ports.json             # CoordonnÃ©es des ports
```

---

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis

- Docker & Docker Compose installÃ©s
- 16 GB RAM minimum
- 20 GB espace disque
- SBT 1.9+ (pour compiler Scala)


| Composant | Technologie | Version | RÃ´le |
|-----------|-------------|---------|------|
| **Langage de Programmation** | Scala | 2.12 | Code mÃ©tier, simulations, traitement |
| **Streaming** | Apache Spark | 3.5.0 | Traitement temps rÃ©el et batch |
| **Message Broker** | Apache Kafka | 3.5.0 | Ingestion en temps rÃ©el, topics distribuÃ©s |
| **SystÃ¨me Fichiers** | HDFS (Hadoop) | 3.2.1 | Stockage distribuÃ©, rÃ©pliquÃ©, fault-tolerant |
| **RequÃªtes SQL** | Impala + Hive | 2.5.6 | RequÃªtes SQL sur donnÃ©es massives |
| **Spark SQL** | Built-in | 3.5.0 | RequÃªtes SQL additionnelles |
| **Build Tool** | SBT | 1.9.2 | Compilation, gestion dÃ©pendances Scala |
| **Orchestration** | Docker Compose | 1.29+ | DÃ©ploiement services conteneurisÃ©s |
| **Visualisation** | Jupyter Notebook | 7.0+ | Analyses interactives en Python/PySpark |
| **Format DonnÃ©es** | Parquet | - | Compression, performance, schÃ©ma |
| **SÃ©rialisation** | JSON | - | Messages Kafka |

---





### Fichiers ClÃ©s DÃ©taillÃ©s

#### 1. **docker-compose.yml**
Services conteneurisÃ©s :
- **zookeeper** : Coordination Kafka
- **kafka** : 3 brokers (replication)
- **namenode** : HDFS NameNode
- **datanode** : HDFS DataNode (stockage)
- **spark-master** : Driver Spark
- **spark-worker** : Executors Spark
- **impala-server** : Moteur requÃªtes SQL
- **hive-metastore** : Metadata Hive
- **jupyter** : Notebooks interactifs

#### 2. **build.sbt**
```scala
name := "MaritimeTracking"
version := "1.0"
scalaVersion := "2.12.15"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.5.0",
  "org.apache.spark" %% "spark-streaming" % "3.5.0",
  "org.apache.spark" %% "spark-sql" % "3.5.0",
  "org.apache.kafka" %% "kafka-clients" % "3.5.0",
  "com.google.code.gson" % "gson" % "2.10.1",
  "org.scalatest" %% "scalatest" % "3.2.15" % Test
)

assemblyMergeStrategy in assembly := {
  case "application.conf" => MergeStrategy.concat
  case _ => (assemblyMergeStrategy in assembly).value
}
```

---

## ğŸš€ Installation et DÃ©marrage DÃ©taillÃ©

### PrÃ©requis SystÃ¨me

- **Docker & Docker Compose** : v1.29+ (ou Docker Desktop)
- **MÃ©moire RAM** : 16 GB minimum (recommandÃ© 24 GB)
- **Espace Disque** : 50 GB libres (donnÃ©es + images Docker)
- **CPU** : 4+ cores (8+ recommandÃ©)
- **SBT** : 1.9+ (pour compilation Scala locale)
- **Java** : JDK 11+ (gÃ©nÃ©ralement fourni par Docker)
- **Git** : Pour clonage du repo
- **SystÃ¨me** : Linux, macOS ou Windows (avec WSL2 recommandÃ©)

### Ã‰tape 1 : Configuration Initiale

#### 1.1 Cloner le Projet

```bash
# SSH (si configurÃ©)
git clone git@github.com:your-repo/maritime-tracking.git

# HTTPS
git clone https://github.com/your-repo/maritime-tracking.git

cd maritime-tracking
```

#### 1.2 Rendre les Scripts ExÃ©cutables

```bash
chmod +x scripts/*.sh
```

#### 1.3 VÃ©rifier Docker

```bash
docker --version      # v20.10+
docker-compose --version  # v1.29+
docker ps            # VÃ©rifier accÃ¨s Docker daemon
```

### Ã‰tape 2 : DÃ©marrer l'Infrastructure Docker

```bash
# Lancer tous les services
./scripts/setup-and-run.sh

# OU manuellement:
docker-compose up -d

# Attendre 30-60 secondes pour dÃ©marrage complet
sleep 60

# VÃ©rifier les services
docker-compose ps
```

RÃ©sultat attendu :
```
CONTAINER ID   IMAGE              STATUS
abc123def      confluentinc/cp-kafka         Up 5 minutes
def456ghi      bde2020/hadoop-namenode       Up 5 minutes
ghi789jkl      bde2020/spark-master          Up 5 minutes
jkl012mno      jupyter/pyspark-notebook      Up 5 minutes
...
```

### Ã‰tape 3 : Compiler le Code Scala

```bash
cd scala-app

# Compilation et crÃ©ation JAR
sbt clean compile assembly

# Cela crÃ©e: target/scala-2.12/MaritimeTracking-assembly-1.0.jar
# (~2-3 minutes)

cd ..
```

### Ã‰tape 4 : Lancer le Pipeline

#### 4.1 DÃ©marrer le Producer Kafka

```bash
# Terminal 1 : Data Generation + Kafka Producer
NUM_SHIPS=10 INTERVAL=2 ./scripts/run-producer.sh
```

**ParamÃ¨tres configurable** :
- `NUM_SHIPS` : Nombre de navires Ã  simuler (dÃ©faut: 5)
- `INTERVAL` : Interval entre messages en secondes (dÃ©faut: 5)
- `KAFKA_BROKERS` : Adresse Kafka (dÃ©faut: localhost:9092)

**Sortie attendue** :
```
ğŸš¢ Maritime Data Producer
Initializing 10 ships...
â”œâ”€ SHIP_001: Tanger â†’ Marseille
â”œâ”€ SHIP_002: Barcelona â†’ Alger
â”œâ”€ SHIP_003: AthÃ¨nes â†’ Naples
â””â”€ ...

Starting data generation (interval: 2 seconds)...

âœ“ Message sent to maritime-tracking [SHIP_001]: offset=1234, partition=0
âœ“ Message sent to maritime-tracking [SHIP_002]: offset=1235, partition=1
âš ï¸  ALERT: SHIP_003 - Low Fuel! (9500L / 100000L capacity)
âœ“ Message sent to maritime-alerts [SHIP_003]: offset=456, partition=0

âœ“ Timestamp: 2025-12-25 14:32:45
...
```

#### 4.2 DÃ©marrer Spark Streaming

Dans un **nouveau terminal** :

```bash
# Terminal 2 : Spark Streaming
./scripts/run-streaming.sh
```

**Sortie attendue** :
```
25/12/25 14:35:00 INFO SparkContext: Running Spark version 3.5.0
...
25/12/25 14:35:15 INFO StreamingContext: StreamingContext started
Processing batch at time ...
- Batch 001: 150 records processed (10 ships, 5 min window)
  â€¢ Aggregated data written to HDFS
  â€¢ Anomalies detected: 2 (SHIP_003: low fuel, SHIP_007: high temp)
  â€¢ Latency: 2.34 seconds
- Batch 002: 145 records processed
  ...
```

**AccÃ©der aux UIs** :
- Spark Master: http://localhost:8080
- Spark Jobs: http://localhost:4040
- HDFS NameNode: http://localhost:9870

#### 4.3 Lancer les Analyses Batch

AprÃ¨s 5-10 minutes de donnÃ©es streaming :

```bash
# Terminal 3 : Spark Batch Analysis
./scripts/run-batch.sh
```

**Sortie attendue** :
```
Starting Spark Batch Analysis...
Loading data from HDFS...
â”œâ”€ raw_data: 1250 records
â”œâ”€ aggregated: 250 records
â””â”€ anomalies: 15 records

Computing ship statistics...
â”œâ”€ Ship efficiency calculation
â”œâ”€ Route performance analysis
â”œâ”€ Temporal patterns
â””â”€ Maintenance risk scores

Writing results to HDFS...
âœ“ ship_statistics written
âœ“ route_performance written
âœ“ temporal_analysis written
âœ“ weather_impact written
âœ“ anomalies_detected written
âœ“ maintenance_prediction written

Analysis complete in 45.32 seconds
```

---

## ğŸ“Š Analyses et RÃ©sultats DÃ©taillÃ©s

### 1. DonnÃ©es Temps RÃ©el (Spark Streaming)

Traitement continu par micro-batch (2 secondes) :

#### 1.1 AgrÃ©gations FenÃªtrÃ©es (5 minutes)

**Calculs par navire** :
- Vitesse moyenne (nÅ“uds)
- Consommation carburant cumulative (litres)
- Distance parcourue (formule Haversine)
- Direction dominante
- Nombre d'anomalies dÃ©tectÃ©es

**Stockage** :
```
hdfs://namenode:9000/maritime/aggregated/
â”œâ”€ date=2025-12-25/
â”‚  â”œâ”€ hour=14/
â”‚  â”‚  â”œâ”€ navire_id=SHIP_001/
â”‚  â”‚  â”‚  â””â”€ 14_30_00.parquet
â”‚  â”‚  â”œâ”€ navire_id=SHIP_002/
â”‚  â”‚  â””â”€ ...
â”‚  â””â”€ hour=15/
â””â”€ ...
```

#### 1.2 DÃ©tection Anomalies

RÃ¨gles prÃ©dÃ©finies :
| Anomalie | Condition | SÃ©vÃ©ritÃ© |
|----------|-----------|----------|
| `LOW_FUEL` | Carburant < 20% de capacitÃ© | ğŸŸ  Moyen |
| `HIGH_TEMP` | TempÃ©rature moteur > 85Â°C | ğŸ”´ Ã‰levÃ© |
| `ABNORMAL_SPEED` | Vitesse < 2 ou > 25 nÅ“uds | ğŸŸ  Moyen |
| `LOST_SIGNAL` | Pas de message depuis 5 min | ğŸ”´ Ã‰levÃ© |
| `OFF_COURSE` | DÃ©viation > 20Â° par rapport route | ğŸŸ¡ Faible |

**Output** :
```
hdfs://namenode:9000/maritime/anomalies/
â”œâ”€ alertes-2025-12-25.parquet
â”œâ”€ alertes-2025-12-26.parquet
â””â”€ ...
```

#### 1.3 Calcul ETA (Estimated Time of Arrival)

```
ETA = (Distance restante) / (Vitesse moyenne rÃ©cente)
```

IntÃ¨gre :
- Position actuelle vs port de destination
- Vitesse moyenne derniÃ¨res 30 minutes
- Ajustements mÃ©tÃ©o (ralentissement estimÃ©)
- Buffers de sÃ©curitÃ© (5-10%)

**Mis Ã  jour** : Chaque 5 minutes

---

### 2. Analyses Batch (Spark SQL)


| Analyse | Description | Fichier |
|---------|-------------|---------|
| **ship_statistics** | Stats par navire (vitesse, consommation, efficacitÃ©) | `/maritime/analysis/ship_statistics` |
| **route_performance** | Performance par route (temps, distance, conso) | `/maritime/analysis/route_performance` |
| **temporal_analysis** | Ã‰volution par heure de la journÃ©e | `/maritime/analysis/temporal_analysis` |
| **weather_impact** | Impact mÃ©tÃ©o sur performance | `/maritime/analysis/weather_impact` |
| **anomalies_detected** | Historique des anomalies | `/maritime/analysis/anomalies_detected` |
| **maintenance_prediction** | Score de risque maintenance | `/maritime/analysis/maintenance_prediction` |

---

## ğŸ—„ï¸ RequÃªtes SQL - Impala/Hive

### CrÃ©er les Tables

```bash
# Copier script SQL dans conteneur Hive
docker cp sql/create-tables.sql hive-metastore:/tmp/

# ExÃ©cuter
docker exec -it hive-metastore beeline -f /tmp/create-tables.sql
```

### CrÃ©ation Tables DÃ©taillÃ©e

#### Table Principale : maritime_raw_data

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS maritime_raw_data (
  timestamp STRING,
  navire_id STRING,
  latitude DOUBLE,
  longitude DOUBLE,
  vitesse_noeuds DOUBLE,
  cap_degres DOUBLE,
  carburant_litres DOUBLE,
  temperature_moteur_celsius DOUBLE,
  rpm_moteur INT,
  poids_cargo_tonnes INT,
  meteo STRING,
  anomalies ARRAY<STRING>
)
PARTITIONED BY (
  date_partition STRING,
  hour_partition INT
)
STORED AS PARQUET
LOCATION '/maritime/raw_data/'
TBLPROPERTIES ("classification"="parquet");
```

#### Vues Analytiques

```sql
-- Vue positions actuelles
CREATE VIEW v_current_positions AS
SELECT 
  navire_id,
  MAX(timestamp) as dernier_update,
  LAST(latitude) as latitude,
  LAST(longitude) as longitude,
  LAST(vitesse_noeuds) as vitesse_actuelle,
  LAST(carburant_litres) as carburant_actuel,
  LAST(temperature_moteur_celsius) as temp_actuelle
FROM maritime_raw_data
GROUP BY navire_id;

-- Vue navires en alerte
CREATE VIEW v_ships_requiring_attention AS
SELECT 
  navire_id,
  timestamp,
  anomalie,
  CASE 
    WHEN anomalie = 'LOW_FUEL' THEN 'Carburant critique'
    WHEN anomalie = 'HIGH_TEMP' THEN 'Moteur surchauffÃ©'
    WHEN anomalie = 'ABNORMAL_SPEED' THEN 'Vitesse anormale'
    ELSE 'Autre anomalie'
  END as description
FROM maritime_anomalies
WHERE timestamp > DATE_SUB(NOW(), 1)
ORDER BY timestamp DESC;

-- Vue efficacitÃ© routes
CREATE VIEW v_route_efficiency AS
SELECT 
  port_depart,
  port_arrivee,
  AVG(distance_nm / NULLIF(carburant_consomme, 0)) as efficacite_moyenne,
  COUNT(*) as nb_traversees,
  AVG(temps_heures) as temps_moyen
FROM maritime_completed_routes
GROUP BY port_depart, port_arrivee;
```

### RequÃªtes Analytiques Courantes

#### 1. Suivi Flotte Temps RÃ©el

```sql
-- Tous les navires et leur status
SELECT 
  navire_id,
  latitude,
  longitude,
  vitesse_noeuds,
  carburant_litres,
  temperature_moteur_celsius,
  CASE 
    WHEN carburant_litres < 20000 THEN 'CRITIQUE'
    WHEN carburant_litres < 40000 THEN 'BAS'
    ELSE 'OK'
  END as fuel_status,
  dernier_update
FROM v_current_positions
ORDER BY dernier_update DESC
LIMIT 50;
```

#### 2. Navires en Maintenance Urgente

```sql
SELECT 
  navire_id,
  COUNT(*) as anomalies_count,
  COLLECT_SET(anomalie) as types_anomalies,
  MAX(temperature_moteur_celsius) as temp_max_recente,
  AVG(rpm_moteur) as rpm_moyen
FROM maritime_raw_data
WHERE timestamp > DATE_SUB(NOW(), INTERVAL 24 HOUR)
GROUP BY navire_id
HAVING COUNT(*) > 5
ORDER BY anomalies_count DESC;
```

#### 3. Top 10 Routes Plus Rapides

```sql
SELECT 
  port_depart,
  port_arrivee,
  ROUND(AVG(distance_nm), 2) as distance_km,
  ROUND(AVG(temps_estime_heures), 2) as temps_moyen_heures,
  ROUND(AVG(vitesse_noeuds), 2) as vitesse_moyenne
FROM maritime_aggregated
WHERE port_depart IS NOT NULL AND port_arrivee IS NOT NULL
GROUP BY port_depart, port_arrivee
ORDER BY temps_estime_heures ASC
LIMIT 10;
```

#### 4. Consommation Carburant par Navire (Dernier 7 jours)

```sql
SELECT 
  navire_id,
  DATE(timestamp) as date_voyage,
  ROUND(SUM(carburant_consomme), 0) as total_litre,
  ROUND(SUM(distance_parcourue), 0) as distance_nm,
  ROUND(SUM(carburant_consomme) / SUM(distance_parcourue) * 100, 2) as litres_par_100nm
FROM maritime_raw_data
WHERE timestamp > DATE_SUB(NOW(), 7)
GROUP BY navire_id, DATE(timestamp)
ORDER BY navire_id, date_voyage;
```

#### 5. Distribution Anomalies par Type

```sql
SELECT 
  anomalie,
  COUNT(*) as occurrences,
  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 2) as pourcentage,
  COUNT(DISTINCT navire_id) as nb_navires_affectes
FROM maritime_anomalies
WHERE timestamp > DATE_SUB(NOW(), 30)
GROUP BY anomalie
ORDER BY occurrences DESC;
```

---

## ğŸ“ˆ Visualisation avec Jupyter

### AccÃ¨s Ã  Jupyter

```bash
# URL d'accÃ¨s
open http://localhost:8888

# Token d'authentification (premiÃ¨re connexion)
docker logs jupyter | grep token
```

### Exemple : Script Analyse ComplÃ¨te

```python
from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
import pandas as pd
import folium
from folium.plugins import HeatMap, MarkerCluster
import numpy as np

# ============= 1. INITIALISER SPARK =============
spark = SparkSession.builder \
    .appName("MaritimeAnalysis") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .config("spark.sql.shuffle.partitions", "16") \
    .getOrCreate()

# ============= 2. CHARGER DONNÃ‰ES =============
# DonnÃ©es brutes
raw_df = spark.read.parquet("hdfs://namenode:9000/maritime/raw_data")

# DonnÃ©es agrÃ©gÃ©es
agg_df = spark.read.parquet("hdfs://namenode:9000/maritime/aggregated")

# Anomalies
alerts_df = spark.read.parquet("hdfs://namenode:9000/maritime/anomalies")

# ============= 3. STATISTIQUES DESCRIPTIVES =============
print("="*60)
print("STATISTIQUES FLOTTE")
print("="*60)

stats_df = raw_df.groupBy("navire_id").agg(
    F.count("*").alias("nb_records"),
    F.avg("vitesse_noeuds").alias("vitesse_moy"),
    F.max("temperature_moteur_celsius").alias("temp_max"),
    F.sum("carburant_consomme").alias("total_consomme")
)

stats_pdf = stats_df.toPandas()
print(stats_pdf.to_string())

# ============= 4. VISUALISATIONS =============

# 4.1 EfficacitÃ© Ã‰nergÃ©tique
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Histogramme consommation
stats_pdf.plot(x='navire_id', y='total_consomme', kind='bar', ax=axes[0])
axes[0].set_title('Consommation Carburant par Navire (derniÃ¨res 24h)')
axes[0].set_ylabel('Litres')
axes[0].set_xlabel('Navire')

# Scatter vitesse vs consommation
axes[1].scatter(stats_pdf['vitesse_moy'], stats_pdf['total_consomme'])
axes[1].set_title('Vitesse vs Consommation')
axes[1].set_xlabel('Vitesse Moyenne (nÅ“uds)')
axes[1].set_ylabel('Consommation Totale (L)')

plt.tight_layout()
plt.savefig('/tmp/efficiency_analysis.png', dpi=100)
plt.show()

# 4.2 Carte Interactive des Trajectoires
# Charger positions recentes
positions_df = raw_df.select("latitude", "longitude", "navire_id", "timestamp") \
    .orderBy("timestamp", ascending=False) \
    .limit(1000)

positions_pdf = positions_df.toPandas()

# CrÃ©er carte Folium
m = folium.Map(
    location=[37.0, 3.0],  # Centre MÃ©diterranÃ©e
    zoom_start=5,
    tiles='OpenStreetMap'
)

# Ajouter markers pour derniÃ¨re position chaque navire
for ship_id in positions_pdf['navire_id'].unique():
    ship_data = positions_pdf[positions_pdf['navire_id'] == ship_id]
    last_pos = ship_data.iloc[0]
    
    folium.CircleMarker(
        location=[last_pos['latitude'], last_pos['longitude']],
        radius=8,
        popup=f"{ship_id}<br>{last_pos['timestamp']}",
        color='red',
        fill=True,
        fillColor='red'
    ).add_to(m)

# Ajouter heatmap trajectoires
HeatMap(
    positions_pdf[['latitude', 'longitude']].values.tolist(),
    radius=20,
    blur=25
).add_to(m)

m.save('/tmp/maritime_trajectories.html')
print("âœ“ Carte sauvegardÃ©e: /tmp/maritime_trajectories.html")

# 4.3 Timeline Anomalies
alerts_pdf = alerts_df.select(
    "timestamp", "navire_id", "anomalie", "severite"
).toPandas()

alerts_pdf['timestamp'] = pd.to_datetime(alerts_pdf['timestamp'])
alerts_by_hour = alerts_pdf.set_index('timestamp').resample('H').size()

fig, ax = plt.subplots(figsize=(14, 5))
alerts_by_hour.plot(kind='line', ax=ax, marker='o')
ax.set_title('Anomalies DÃ©tectÃ©es par Heure')
ax.set_ylabel('Nombre d\'anomalies')
ax.set_xlabel('Heure')
plt.tight_layout()
plt.savefig('/tmp/anomalies_timeline.png', dpi=100)
plt.show()

# ============= 5. EXPORT RÃ‰SULTATS =============
# Exporter en CSV
stats_pdf.to_csv('/tmp/fleet_statistics.csv', index=False)
alerts_pdf.to_csv('/tmp/alerts_history.csv', index=False)

print("\nâœ“ Analyses complÃ©tÃ©es et exportÃ©es")
print("  - Fleet Statistics: /tmp/fleet_statistics.csv")
print("  - Alerts History: /tmp/alerts_history.csv")
print("  - Trajectories Map: /tmp/maritime_trajectories.html")
```

### Notebooks Disponibles

1. **maritime_analysis.py** (inclus)
   - Vue d'ensemble flotte
   - Analyses temporelles
   - Cartes trajectoires
   - Tableaux bord KPIs

2. **Notebooks personnalisÃ©s** Ã  crÃ©er :
   - PrÃ©dictions maintenance ML
   - Optimisation routes IA
   - Alertes prÃ©dictives
   - Benchmarking navires


---

## ğŸ” Monitoring et Debugging DÃ©taillÃ©

### Interfaces Web

| Service | URL | Port | Description |
|---------|-----|------|-------------|
| **Spark Master** | http://localhost:8080 | 8080 | Ã‰tat cluster, workers, applications |
| **Spark Job UI** | http://localhost:4040 | 4040 | Jobs en cours, stages, tasks |
| **HDFS NameNode** | http://localhost:9870 | 9870 | Exploration HDFS, fichiers, health |
| **Jupyter** | http://localhost:8888 | 8888 | Notebooks interactifs |
| **Kafka Manager** | http://localhost:9000 | 9000 | Gestion topics (optionnel) |
| **Impala** | localhost:21000 | 21000 | Shell requÃªtes (CLI) |

### Commandes Docker Utiles

#### Gestion Services

```bash
# Voir tous les conteneurs
docker-compose ps

# Voir logs en temps rÃ©el
docker-compose logs -f kafka        # Kafka
docker-compose logs -f spark-master # Spark
docker-compose logs -f jupyter      # Jupyter

# RedÃ©marrer un service
docker-compose restart spark-master
docker-compose restart kafka

# ArrÃªter tous les services
docker-compose down

# RedÃ©marrer complet
docker-compose down && docker-compose up -d
```

#### AccÃ¨s Conteneurs

```bash
# Entrer dans un conteneur
docker exec -it spark-master bash
docker exec -it namenode bash
docker exec -it kafka bash

# ExÃ©cuter commande unique
docker exec spark-master spark-submit --version
docker exec namenode hdfs dfs -ls /
```

### Monitoring Kafka

```bash
# Lister tous les topics
docker exec kafka kafka-topics \
    --list \
    --bootstrap-server localhost:9092

# DÃ©tails d'un topic
docker exec kafka kafka-topics \
    --describe \
    --topic maritime-tracking \
    --bootstrap-server localhost:9092

# Consommer messages
docker exec kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic maritime-tracking \
    --from-beginning \
    --max-messages 100

# Consommer alerts seulement
docker exec kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic maritime-alerts \
    --from-beginning

# VÃ©rifier lag consumer groups
docker exec kafka kafka-consumer-groups \
    --bootstrap-server localhost:9092 \
    --group maritime-streaming \
    --describe
```

### Monitoring HDFS

```bash
# Explorer hiÃ©rarchie
docker exec namenode hdfs dfs -ls -h /maritime

# Voir fichiers dÃ©taillÃ©
docker exec namenode hdfs dfs -ls -R /maritime/raw_data

# Espace disque utilisÃ©
docker exec namenode hdfs dfsadmin -report

# SantÃ© cluster
docker exec namenode hdfs dfsadmin -report | grep "Name:"

# Permissions fichiers
docker exec namenode hdfs dfs -stat /maritime

# Copier fichier depuis HDFS
docker exec namenode hdfs dfs -cat /maritime/raw_data/part-*.parquet | head -c 1000

# VÃ©rifier replication
docker exec namenode hdfs fsck /maritime -blocks -locations
```

### Monitoring Spark Streaming

```bash
# Voir jobs en cours
# â†’ Aller Ã  http://localhost:4040/jobs

# Logs dÃ©taillÃ©s streaming
docker logs -f spark-master | grep -i "batch\|processing\|trigger"

# Voir RDD stats
docker exec spark-master spark-shell <<EOF
val rdd = sc.textFile("hdfs://namenode:9000/maritime/raw_data")
println(rdd.count())
EOF
```

---

## ğŸ› ï¸ Architecture Code Scala DÃ©taillÃ©e

### DataGenerator.scala

**ResponsabilitÃ©** : Simulation des navires

```scala
case class Port(name: String, latitude: Double, longitude: Double)

case class Ship(
  id: String,
  portDeparture: Port,
  portArrival: Port,
  capacity: Double = 100000  // litres carburant
)

case class ShipTelemetry(
  timestamp: String,
  navire_id: String,
  latitude: Double,
  longitude: Double,
  vitesse_noeuds: Double,
  cap_degres: Double,
  carburant_litres: Double,
  temperature_moteur_celsius: Double,
  rpm_moteur: Int,
  anomalies: List[String] = List()
)

object DataGenerator {
  def generateTelemetry(ship: Ship, step: Int): ShipTelemetry = {
    // Haversine distance calculation
    val distance = haversineDistance(
      ship.currentLat, ship.currentLong,
      ship.portArrival.latitude, ship.portArrival.longitude
    )
    
    // Simulation dÃ©placement
    val newLat = ship.currentLat + (Random.nextDouble() - 0.5) * 0.01
    val newLong = ship.currentLong + (Random.nextDouble() - 0.5) * 0.01
    
    // Consommation carburant
    val consumption = vitesse_noeuds * 1.5 // litres par heure
    val newFuel = ship.currentFuel - consumption
    
    // DÃ©tection anomalies
    val anomalies = List(
      if (newFuel < 20000) Some("LOW_FUEL") else None,
      if (temperature > 85) Some("HIGH_TEMP") else None
    ).flatten
    
    ShipTelemetry(
      timestamp = System.currentTimeMillis(),
      navire_id = ship.id,
      latitude = newLat,
      longitude = newLong,
      vitesse_noeuds = Random.nextDouble() * 20,
      cap_degres = Random.nextDouble() * 360,
      carburant_litres = newFuel,
      temperature_moteur_celsius = 70 + Random.nextDouble() * 20,
      rpm_moteur = (1000 + Random.nextInt(2000)),
      anomalies = anomalies
    )
  }
  
  private def haversineDistance(
    lat1: Double, lon1: Double,
    lat2: Double, lon2: Double
  ): Double = {
    val R = 6371.0 // Rayon Terre km
    val dLat = Math.toRadians(lat2 - lat1)
    val dLon = Math.toRadians(lon2 - lon1)
    val a = Math.sin(dLat/2) * Math.sin(dLat/2) +
            Math.cos(Math.toRadians(lat1)) * Math.cos(Math.toRadians(lat2)) *
            Math.sin(dLon/2) * Math.sin(dLon/2)
    val c = 2 * Math.asin(Math.sqrt(a))
    R * c * 0.539957 // Convertir en milles nautiques
  }
}
```

### KafkaProducer.scala

**ResponsabilitÃ©** : Envoi messages Kafka

```scala
object KafkaProducer {
  def main(args: Array[String]): Unit = {
    val props = new Properties()
    props.put("bootstrap.servers", "localhost:9092")
    props.put("key.serializer", 
      "org.apache.kafka.common.serialization.StringSerializer")
    props.put("value.serializer",
      "org.apache.kafka.common.serialization.StringSerializer")
    
    val producer = new KafkaProducer[String, String](props)
    
    // GÃ©nÃ©rer et envoyer
    for (i <- 1 to 1000) {
      val telemetry = DataGenerator.generateTelemetry(...)
      val json = serializeToJson(telemetry)
      
      // Topic : maritime-tracking ou maritime-alerts selon anomalies
      val topic = if (telemetry.anomalies.nonEmpty) 
        "maritime-alerts" else "maritime-tracking"
      
      val record = new ProducerRecord[String, String](
        topic,
        telemetry.navire_id,  // Key pour partitionnement
        json                  // Value
      )
      
      producer.send(record)
      Thread.sleep(5000)
    }
    
    producer.close()
  }
}
```

### SparkStreaming.scala

**ResponsabilitÃ©** : Traitement temps rÃ©el

```scala
object SparkStreaming {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("MaritimeStreaming")
      .getOrCreate()
    
    // Lire depuis Kafka
    val kafkaDF = spark
      .readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "kafka:9092")
      .option("subscribe", "maritime-tracking")
      .option("startingOffsets", "latest")
      .load()
    
    // Parser JSON
    val schemaString = "timestamp STRING, navire_id STRING, latitude DOUBLE, ..."
    val schema = StructType.fromDDL(schemaString)
    
    val dataDF = kafkaDF
      .select(from_json(col("value"), schema) as "data")
      .select("data.*")
    
    // AgrÃ©gations fenÃªtrÃ©es 5 minutes
    val aggregations = dataDF
      .withWatermark("timestamp", "10 minutes")
      .groupBy(
        window(col("timestamp"), "5 minutes"),
        col("navire_id")
      )
      .agg(
        avg("vitesse_noeuds").as("vitesse_moyenne"),
        sum("carburant_consomme").as("carburant_cumul"),
        max("temperature_moteur_celsius").as("temp_max"),
        count("*").as("nb_records")
      )
    
    // Ã‰crire HDFS
    aggregations
      .writeStream
      .format("parquet")
      .option("path", "hdfs://namenode:9000/maritime/aggregated")
      .option("checkpointLocation", "hdfs://namenode:9000/.checkpoint")
      .partitionBy("navire_id")
      .mode("append")
      .start()
      .awaitTermination()
  }
}
```

### SparkBatch.scala

**ResponsabilitÃ©** : Analyses batch

```scala
object SparkBatch {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("MaritimeBatch")
      .getOrCreate()
    
    // Charger donnÃ©es HDFS
    val rawDF = spark.read.parquet(
      "hdfs://namenode:9000/maritime/raw_data/*"
    )
    
    // Statistiques navires
    val shipStats = rawDF
      .groupBy("navire_id")
      .agg(
        avg("vitesse_noeuds").as("vitesse_moy"),
        sum("carburant_consomme").as("consomme_total"),
        max("temperature_moteur_celsius").as("temp_max")
      )
    
    shipStats.write
      .mode("overwrite")
      .parquet("hdfs://namenode:9000/maritime/analysis/ship_statistics")
    
    // Performance routes
    val routePerf = rawDF
      .filter(col("port_depart").isNotNull)
      .groupBy("port_depart", "port_arrivee")
      .agg(
        avg("distance_nm").as("distance_moy"),
        avg("temps_heures").as("temps_moyen")
      )
      .orderBy("temps_moyen")
    
    routePerf.write
      .mode("overwrite")
      .parquet("hdfs://namenode:9000/maritime/analysis/route_performance")
  }
}
```

---

## ğŸ“ Points ClÃ©s pour Ã‰valuation

#### Structure RecommandÃ©e

1. **Introduction**
   - Contexte du projet
   - Cas d'usage maritime
   - Objectifs

2. **Architecture**
   - SchÃ©ma du pipeline
   - Justification des choix technologiques
   - Flux de donnÃ©es

3. **ImplÃ©mentation**
   - **GÃ©nÃ©ration des donnÃ©es** (Scala)
     - ModÃ¨le de simulation
     - Formule Haversine pour calculs gÃ©ographiques
   - **Kafka** : Configuration, topics, producer
   - **Spark Streaming** : FenÃªtres de temps, watermarking
   - **HDFS** : Partitionnement, format Parquet
   - **Impala/Spark SQL** : Tables, vues, requÃªtes complexes

4. **Analyses et RÃ©sultats**
   - Statistiques descriptives
   - Visualisations (graphiques, cartes)
   - Insights mÃ©tier

5. **Performances**
   - Throughput Kafka
   - Latence de traitement Spark
   - ScalabilitÃ© du systÃ¨me

6. **Conclusion**
   - Apprentissages
   - Limites du projet
   - AmÃ©liorations futures

### Code Source

Structure Ã  inclure :
```
maritime-tracking-code/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ scala-app/
â”‚   â”œâ”€â”€ build.sbt
â”‚   â””â”€â”€ src/main/scala/maritime/
â”œâ”€â”€ scripts/
â”œâ”€â”€ sql/
â””â”€â”€ notebooks/
```

### Captures d'Ã‰cran

Ã€ inclure dans le rapport :
- âœ… Spark UI montrant les jobs en cours
- âœ… HDFS UI avec l'arborescence des fichiers
- âœ… Logs Kafka montrant les messages
- âœ… RÃ©sultats des requÃªtes SQL
- âœ… Graphiques de visualisation (Jupyter)
- âœ… Carte des trajectoires maritimes

---

## ğŸ“ Points ClÃ©s pour l'Ã‰valuation

### Spark (RDD, DataFrame, Spark SQL, Streaming)

âœ… **RDD** : Manipulation bas niveau dans `DataGenerator`  
âœ… **DataFrame** : Transformations dans `SparkStreaming` et `SparkBatch`  
âœ… **Spark SQL** : RequÃªtes complexes, agrÃ©gations, fenÃªtres  
âœ… **Spark Streaming** : Traitement temps rÃ©el avec watermarking

### Scala

âœ… **Classes case** : `Ship`, `ShipTelemetry`  
âœ… **Pattern matching** : Gestion des anomalies  
âœ… **Fonctions** : Haversine, calcul de cap  
âœ… **IntÃ©gration Spark** : Code idiomatique Scala

### Kafka

âœ… **Producer** : Envoi de messages JSON  
âœ… **Topics** : `maritime-tracking`, `maritime-alerts`  
âœ… **Consumer** : Spark Streaming lit depuis Kafka  
âœ… **Partitionnement** : Par `navire_id`

### Impala/Hive

âœ… **Tables externes** : Sur donnÃ©es Parquet HDFS  
âœ… **Partitionnement** : Par date et navire_id  
âœ… **Vues** : Positions actuelles, alertes, rÃ©sumÃ©s  
âœ… **RequÃªtes distribuÃ©es** : AgrÃ©gations complexes

### HDFS

âœ… **Stockage distribuÃ©** : RÃ©plication, tolÃ©rance aux pannes  
âœ… **Organisation** : HiÃ©rarchie `/maritime/raw_data/`, `/analysis/`  
âœ… **Format** : Parquet pour compression et performance

### Outil Additionnel : Docker

âœ… **Orchestration** : Docker Compose pour tous les services  
âœ… **Networking** : RÃ©seau `maritime-network`  
âœ… **Volumes** : Persistance des donnÃ©es

---

## ğŸ”§ Personnalisation et Extensions

### Ajouter Plus de Navires

```bash
NUM_SHIPS=20 ./scripts/run-producer.sh
```

### Modifier l'Intervalle de GÃ©nÃ©ration

```bash
INTERVAL=5 ./scripts/run-producer.sh  # Toutes les 5 secondes
```

### Ajouter de Nouveaux Ports

Ã‰diter `DataGenerator.scala` :

```scala
val ports = Map(
  "Tanger" -> (35.7595, -5.8340),
  "Marseille" -> (43.2965, 5.3698),
  "AthÃ¨nes" -> (37.9838, 23.7275),  // Nouveau
  "Istamboul" -> (41.0082, 28.9784) // Nouveau
)
```

### Ajouter de Nouvelles MÃ©triques

Dans `ShipTelemetry`, ajouter :

```scala
case class ShipTelemetry(
  // ... champs existants
  pression_atmospherique: Double,
  hauteur_vagues_metres: Double,
  direction_vent_degres: Double
)
```

---

## ğŸ› Troubleshooting

### ProblÃ¨me : Kafka ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker logs kafka

# RedÃ©marrer Zookeeper puis Kafka
docker-compose restart zookeeper
sleep 10
docker-compose restart kafka
```

### ProblÃ¨me : HDFS "Safe Mode"

```bash
docker exec namenode hdfs dfsadmin -safemode leave
```

### ProblÃ¨me : MÃ©moire insuffisante Spark

Dans `docker-compose.yml`, augmenter :

```yaml
spark-master:
  environment:
    - SPARK_DRIVER_MEMORY=4G  # Au lieu de 2G
```

### ProblÃ¨me : Compilation Scala Ã©choue

```bash
cd scala-app
rm -rf target project/target
sbt clean compile
```

---

## ğŸ“š Ressources ComplÃ©mentaires

### Documentation Officielle

- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Apache Kafka](https://kafka.apache.org/documentation/)
- [Apache Hadoop](https://hadoop.apache.org/docs/current/)
- [Scala](https://docs.scala-lang.org/)

### Concepts ClÃ©s

- **Streaming** : Traitement de flux de donnÃ©es en temps rÃ©el
- **Partitionnement** : Division des donnÃ©es pour parallÃ©lisme
- **Watermarking** : Gestion des donnÃ©es en retard (late data)
- **Windowing** : AgrÃ©gations sur fenÃªtres de temps

---

## ğŸ‘¥ Auteur

**Projet Big Data / Data Engineering**  
Module : Big Data Ecosystem  
Technologies : Kafka, Spark, Scala, HDFS, Impala

---

## ğŸ“„ Licence

Projet acadÃ©mique - Utilisation libre pour apprentissage.

---

---

## ğŸ“ Points ClÃ©s pour Ã‰valuation

### Spark (RDD, DataFrame, Spark SQL, Streaming)

#### RDD (Resilient Distributed Datasets)

âœ… **Utilisation dans DataGenerator** :
- CrÃ©ation RDD navires parallÃ©lisÃ©s
- Transformations map/flatMap pour telemetry
- Persistence en cache pour rÃ©utilisation

```scala
val shipsRDD = sc.parallelize(ships).cache()
val telemetryRDD = shipsRDD.flatMap(ship => 
  (0 until 1000).map(i => generateTelemetry(ship, i))
)
```

#### DataFrames & Spark SQL

âœ… **SparkStreaming & SparkBatch** :
- Parsing JSON â†’ Structured Streaming DataFrame
- AgrÃ©gations window, groupBy, agg
- Optimisations Catalyst optimizer
- Partitionnement intelligent

```scala
val df = spark.read.json(jsonRDD)
df.createOrReplaceTempView("maritime_data")
spark.sql("SELECT navire_id, AVG(vitesse_noeuds) FROM maritime_data GROUP BY navire_id")
```

#### Spark SQL

âœ… **RequÃªtes distribuÃ©es** :
- Joins multi-tables
- Window functions (ROW_NUMBER, LAG, LEAD)
- AgrÃ©gations complexes
- Optimisation query plans

```scala
df.filter(col("carburant_litres") < 20000)
  .groupBy("navire_id")
  .agg(count("*").as("nb_alerts"))
  .orderBy(desc("nb_alerts"))
```

#### Spark Streaming

âœ… **Traitement temps rÃ©el** :
- Micro-batch processing (2 sec intervals)
- Windowed aggregations (5 min windows)
- Watermarking pour late data (10 min tolerance)
- Stateful operations (agregation state)

```scala
df.withWatermark("timestamp", "10 minutes")
  .groupBy(window(col("timestamp"), "5 minutes"), col("navire_id"))
  .agg(avg("vitesse_noeuds"))
```

### Scala

âœ… **Concepts avancÃ©s** :
- **Case classes** : ShipTelemetry, Ship, Port
- **Pattern matching** : DÃ©tection anomalies (match/case)
- **Implicits & Type classes** : SÃ©rialisation JSON
- **Higher-order functions** : map, filter, fold, foldLeft
- **Collections API** : List, Map, Set, Seq, Iterator
- **Functional programming** : Pure functions, composition, recursion
- **Option/Try/Either** : Error handling sans exceptions
- **For comprehensions** : Abstractions monadic

```scala
case class ShipTelemetry(...) // Case class
val telemetry = ShipTelemetry(...)
telemetry match {  // Pattern matching
  case ShipTelemetry(_, _, _, _, _, _, f, _, _, alerts) if f < 20000 =>
    println("Low fuel alert!")
  case _ => ()
}
```

### Kafka

âœ… **Concepts clÃ©s** :
- **Producers** : DataGenerator â†’ Topic maritime-tracking/maritime-alerts
- **Topics** : 2 topics (tracking + alerts), 8 + 4 partitions
- **Partitioning** : Par navire_id (key) â†’ garantit ordre par navire
- **Consumers** : Spark Streaming Consumer Group
- **Replication Factor** : 1 (dÃ©veloppement) Ã  3 (production)
- **Brokers** : Cluster Kafka haute disponibilitÃ©
- **Serialization** : JSON format messages

```
Producer (Ship Data) â†’ [Topic: maritime-tracking] â† Spark Streaming Consumer
                      â””â”€ 8 partitions (SHIP_001 â†’ P0, SHIP_002 â†’ P1, etc)
```

### Impala/Hive

âœ… **Big Data SQL** :
- **External Tables** : Sur donnÃ©es Parquet HDFS
- **Partitioning** : Par date, heure, navire_id â†’ Pruning automatique
- **Materialized Views** : v_current_positions, v_ships_requiring_attention
- **Distributed Queries** : ExÃ©cution parallÃ¨le sur cluster
- **Complex Joins** : Multi-table analytics, Self-joins
- **Window Functions** : ROW_NUMBER, RANK, DENSE_RANK, LAG, LEAD
- **Aggregations** : GROUP BY avec HAVING, SUM OVER, AVG OVER

```sql
SELECT 
  navire_id,
  timestamp,
  carburant_litres,
  LAG(carburant_litres) OVER (PARTITION BY navire_id ORDER BY timestamp) as prev_fuel
FROM maritime_raw_data
WHERE carburant_litres < LAG(carburant_litres) OVER (...)
```

### HDFS (Hadoop)

âœ… **Distributed File System** :
- **NameNode** : MÃ©tadata, namespace management, file system hierarchy
- **DataNodes** : Stockage blocs (blocs 128/256 MB)
- **Replication** : TolÃ©rance 2 nÅ“uds dÃ©faillants (factor=3)
- **Rack-awareness** : Placement blocs racks diffÃ©rents
- **Partitioning Strategy** :
  ```
  /maritime/raw_data/date=2025-12-25/hour=14/ship_001/part-00000.parquet
  ```
  â†’ Partition pruning sur date/hour/navire
- **Format** : Parquet compression (Snappy, LZO, GZIP)
- **Fault Tolerance** : RÃ©plication automatique, heartbeat, re-replication
- **Data Locality** : Computation moves to data principle

### Docker & Orchestration

âœ… **Containerization** :
- **docker-compose.yml** : DÃ©finit 9+ services et leur configuration
- **Networking** : Conteneurs communiquent via bridge network `maritime-network`
- **Volumes** : Persistance donnÃ©es (/data, /logs, /checkpoint)
- **Health checks** : Validation services prÃªts avant dÃ©marrage
- **Resource management** : Limites CPU/RAM par service
- **Logging** : CentralisÃ© avec docker logs

---

## ğŸ”§ Configuration et Personnalisation

### Augmenter Nombre de Navires

```bash
NUM_SHIPS=50 ./scripts/run-producer.sh
```

Modifie aussi `scala-app/src/main/scala/maritime/DataGenerator.scala` :

```scala
val ports = Map(
  "Tanger" -> (35.7595, -5.8340),
  "Marseille" -> (43.2965, 5.3698),
  "Valencia" -> (39.4699, -0.3763),
  "Barcelona" -> (41.3851, 2.1734),
  "Alger" -> (36.7372, 3.0588),
  "AthÃ¨nes" -> (37.9838, 23.7275),
  "Istamboul" -> (41.0082, 28.9784),  // NOUVEAU
  "Naples" -> (40.8518, 14.2681),     // NOUVEAU
  "Palma" -> (39.5696, 2.6502)        // NOUVEAU
)
```

### Modifier Intervalle GÃ©nÃ©ration DonnÃ©es

```bash
INTERVAL=2 ./scripts/run-producer.sh  # 2 secondes entre messages
INTERVAL=1 ./scripts/run-producer.sh  # 1 seconde (dÃ©bit max)
```

### Ajouter Nouvelles MÃ©triques

Dans `ShipTelemetry` :

```scala
case class ShipTelemetry(
  // ... existants ...
  pression_atmospherique_hpa: Double,
  hauteur_vagues_metres: Double,
  direction_vent_degres: Double,
  couvert_nuageux_percent: Int
)
```

Puis ajouter generation dans `DataGenerator`:

```scala
pression_atmospherique_hpa = 1013.25 + Random.nextGaussian() * 2,
hauteur_vagues_metres = 0.5 + Random.nextDouble() * 4,
direction_vent_degres = Random.nextDouble() * 360,
couvert_nuageux_percent = Random.nextInt(101)
```

### Augmenter RÃ©tention HDFS

Modifier `docker-compose.yml` :

```yaml
namenode:
  environment:
    - HDFS_CONF_dfs_namenode_safemode_threshold_pct: 0.99
    - HDFS_CONF_dfs_replication: 2  # Au lieu de 3 si espace limitÃ©
```

---

## ğŸ› Troubleshooting AvancÃ©

### ProblÃ¨me : Kafka ne reÃ§oit pas de messages

```bash
# VÃ©rifier producer logs
docker logs -f spark-master | grep KafkaProducer

# VÃ©rifier broker
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# VÃ©rifier topics existent
docker exec kafka kafka-topics \
    --create --if-not-exists \
    --bootstrap-server localhost:9092 \
    --topic maritime-tracking \
    --partitions 8 \
    --replication-factor 1

# Tester producer simple
docker exec kafka kafka-console-producer \
    --broker-list localhost:9092 \
    --topic maritime-tracking <<EOF
{"test": "message"}
EOF
```

### ProblÃ¨me : Spark Streaming plante

```bash
# VÃ©rifier mÃ©moire
docker stats spark-master

# Augmenter dans docker-compose.yml
spark-master:
  environment:
    - SPARK_DRIVER_MEMORY=4g     # Au lieu de 2g
    - SPARK_EXECUTOR_MEMORY=4g   # Au lieu de 2g

# RedÃ©marrer
docker-compose restart spark-master
```

### ProblÃ¨me : HDFS "Safe Mode"

```bash
# VÃ©rifier status
docker exec namenode hdfs dfsadmin -safemode get

# Quitter safe mode
docker exec namenode hdfs dfsadmin -safemode leave

# VÃ©rifier health
docker exec namenode hdfs dfsadmin -report
```

### ProblÃ¨me : MÃ©moire cluster insuffisante

```bash
# VÃ©rifier utilisation
docker stats

# RÃ©duire batch size
# Dans docker-compose.yml
spark-worker:
  environment:
    - SPARK_EXECUTOR_CORES=1  # Au lieu de 2
    - SPARK_EXECUTOR_MEMORY=1g  # Au lieu de 2g

# Ou rÃ©duire NUM_SHIPS
NUM_SHIPS=5 ./scripts/run-producer.sh
```

### ProblÃ¨me : RequÃªtes SQL lentes

```bash
# VÃ©rifier partitions
docker exec namenode hdfs dfs -ls -h /maritime/raw_data/

# RecrÃ©er tables avec meilleures partitions
# Dans sql/create-tables.sql :
PARTITIONED BY (
  date_partition STRING,
  hour_partition INT,
  navire_id STRING  # Ajouter partition
)

# Analyser query plan
spark-sql> EXPLAIN SELECT ... ;

# Augmenter parallel processes
# Dans docker-compose.yml
spark-master:
  environment:
    - SPARK_SQL_SHUFFLE_PARTITIONS: 32
```

---

## ğŸ“š Ressources & RÃ©fÃ©rences

### Documentation Officielle

- **Apache Spark** : https://spark.apache.org/docs/latest/
- **Apache Kafka** : https://kafka.apache.org/documentation/
- **Apache Hadoop/HDFS** : https://hadoop.apache.org/docs/current/
- **Scala** : https://docs.scala-lang.org/
- **Impala** : https://impala.apache.org/
- **Folium Maps** : https://python-visualization.github.io/folium/

### Concepts Ã  MaÃ®triser

| Concept | Description | Ressource |
|---------|-------------|-----------|
| **Streaming** | Traitement donnÃ©es temps rÃ©el | [Spark Streaming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html) |
| **Window Functions** | AgrÃ©gations temporelles | [Spark Window Functions](https://spark.apache.org/docs/latest/sql-ref-window-functions.html) |
| **Watermarking** | Gestion donnÃ©es tardives | [Spark Watermarking](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-event-time-and-late-data) |
| **Partitioning** | Optimisation parallÃ©lisme | [HDFS Partitioning](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) |
| **Haversine Formula** | Distance gÃ©ographique | [Haversine Distance](https://en.wikipedia.org/wiki/Haversine_formula) |

### Tutoriels RecommandÃ©s

1. Introduction Spark Streaming (30 min)
2. Kafka Producer/Consumer Pattern (45 min)
3. HDFS Architecture & Best Practices (1 h)
4. Impala Query Optimization (1 h)
5. Scala for Big Data (2 h)

---

## ğŸ‘¥ Auteur et Contexte

**Projet Big Data / Data Engineering**  
**Module** : Big Data Ecosystem (Kafka, Spark, Scala, HDFS, Impala)  
**Date** : DÃ©cembre 2025  
**Technologies** : Kafka, Spark, Scala, HDFS, Impala, Docker, Jupyter

---

## ğŸ“„ Licence

Projet acadÃ©mique - Utilisation libre pour apprentissage et fin d'Ã©tudes.

---

## âœ… Checklist Finale

Avant de rendre votre projet, vÃ©rifiez :

- [ ] Tous les services Docker dÃ©marrent correctement (`docker-compose ps`)
- [ ] Le producer Kafka gÃ©nÃ¨re des donnÃ©es (vÃ©rifier topic avec kafka-console-consumer)
- [ ] Spark Streaming traite les flux en temps rÃ©el (Spark UI http://localhost:4040)
- [ ] Les donnÃ©es sont Ã©crites dans HDFS (vÃ©rifier avec `hdfs dfs -ls /maritime`)
- [ ] L'analyse batch produit des rÃ©sultats (vÃ©rifier `/maritime/analysis/`)
- [ ] Les tables Impala/Hive sont crÃ©Ã©es (exÃ©cuter `CREATE TABLE` scripts)
- [ ] Les requÃªtes SQL s'exÃ©cutent sans erreur (tester sur Impala shell)
- [ ] Le rapport documentation est complet (10-20 pages minimum)
- [ ] Le code est bien commentÃ© et proprement formatÃ© (indentation, noms explicites)
- [ ] Les captures d'Ã©cran sont incluses (Spark UI, HDFS, Kafka, Jupyter, RÃ©sultats SQL)
- [ ] Un README explique comment lancer le projet (celui-ci!)
- [ ] Le dÃ©pÃ´t Git contient tous les fichiers nÃ©cessaires (pas de dossiers vides)
- [ ] Les scripts sont tous exÃ©cutables (`chmod +x scripts/*.sh`)
- [ ] La compilation Scala rÃ©ussit sans erreurs (`sbt clean compile assembly`)
- [ ] Au moins 1000 enregistrements sont traitÃ©s pour dÃ©monstration
- [ ] Les analyses batch produisent des rÃ©sultats exploitables

---

## ğŸš€ Quick Start Guide

Pour dÃ©marrer rapidement :

```bash
# 1. Cloner et configuration
git clone <repo>
cd maritime-tracking
chmod +x scripts/*.sh

# 2. DÃ©marrer infrastructure
docker-compose up -d
sleep 60

# 3. Compiler code Scala
cd scala-app && sbt clean compile assembly && cd ..

# 4. Lancer producer (Terminal 1)
NUM_SHIPS=10 INTERVAL=2 ./scripts/run-producer.sh

# 5. Lancer Spark Streaming (Terminal 2)
./scripts/run-streaming.sh

# 6. Lancer analyses batch (Terminal 3, aprÃ¨s 5 min)
./scripts/run-batch.sh

# 7. AccÃ©der visualisations
# - Spark: http://localhost:4040
# - HDFS: http://localhost:9870
# - Jupyter: http://localhost:8888
```

---

**ğŸš¢ Bon courage pour votre projet Big Data ! Consultez ce README pour tous les dÃ©tails. Amusez-vous ! ğŸš€**