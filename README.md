# ğŸš¢ Maritime Tracking System - Big Data Project

## ğŸ“‹ Description du Projet

SystÃ¨me de suivi en temps rÃ©el d'une flotte maritime utilisant un pipeline Big Data complet. Ce projet dÃ©montre l'intÃ©gration de technologies distribuÃ©es pour l'ingestion, le traitement et l'analyse de donnÃ©es IoT.

### ğŸ¯ Cas d'Usage

Suivi de navires commerciaux sur des routes maritimes mÃ©diterranÃ©ennes avec :
- **TÃ©lÃ©mÃ©trie en temps rÃ©el** : Position GPS, vitesse, cap, consommation carburant
- **Alertes automatiques** : Carburant bas, anomalies moteur, conditions mÃ©tÃ©o
- **Analyses prÃ©dictives** : ETA (Estimated Time of Arrival), maintenance prÃ©dictive
- **Optimisation de routes** : Analyse des performances par trajectoire

---

## ğŸ—ï¸ Architecture du Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GÃ©nÃ©rateur     â”‚  Simulation de navires en Scala
â”‚  Scala          â”‚  (DataGenerator.scala)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka          â”‚  Ingestion temps rÃ©el
â”‚  Producer       â”‚  Topics: maritime-tracking, maritime-alerts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark          â”‚  Traitement streaming + batch
â”‚  Streaming      â”‚  - AgrÃ©gations fenÃªtrÃ©es (5 min)
â”‚                 â”‚  - DÃ©tection anomalies
â”‚                 â”‚  - Calcul ETA
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HDFS           â”‚  Stockage distribuÃ©
â”‚                 â”‚  - DonnÃ©es brutes (partitionnÃ©es)
â”‚                 â”‚  - DonnÃ©es agrÃ©gÃ©es
â”‚                 â”‚  - Analyses
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Impala/Hive    â”‚  RequÃªtes SQL distribuÃ©es
â”‚  + Spark SQL    â”‚  - Tables partitionnÃ©es
â”‚                 â”‚  - Vues analytiques
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visualisation  â”‚  Jupyter Notebook
â”‚                 â”‚  - Cartes des trajectoires
â”‚                 â”‚  - Dashboards analytiques
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technologies UtilisÃ©es

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **Ingestion** | Kafka 3.5.0 | Message broker temps rÃ©el |
| **Traitement** | Spark 3.5.0 (Scala 2.12) | Streaming + Batch processing |
| **Stockage** | HDFS (Hadoop 3.2.1) | SystÃ¨me de fichiers distribuÃ© |
| **SQL** | Hive/Impala + Spark SQL | RequÃªtes sur donnÃ©es massives |
| **Orchestration** | Docker Compose | Gestion des conteneurs |
| **Visualisation** | Jupyter (PySpark) | Notebooks interactifs |

---

## ğŸ“¦ Structure du Projet

```
maritime-tracking/
â”œâ”€â”€ docker-compose.yml          # Configuration des services
â”œâ”€â”€ README.md                   # Ce fichier
â”‚
â”œâ”€â”€ scala-app/                  # Application Scala
â”‚   â”œâ”€â”€ build.sbt              # Configuration SBT
â”‚   â”œâ”€â”€ project/
â”‚   â”‚   â”œâ”€â”€ build.properties
â”‚   â”‚   â””â”€â”€ plugins.sbt
â”‚   â””â”€â”€ src/main/scala/maritime/
â”‚       â”œâ”€â”€ DataGenerator.scala      # Simulation navires
â”‚       â”œâ”€â”€ KafkaProducer.scala      # Producer Kafka
â”‚       â”œâ”€â”€ SparkStreaming.scala     # Traitement temps rÃ©el
â”‚       â””â”€â”€ SparkBatch.scala         # Analyses batch
â”‚
â”œâ”€â”€ scripts/                    # Scripts d'automatisation
â”‚   â”œâ”€â”€ setup-and-run.sh       # Installation complÃ¨te
â”‚   â”œâ”€â”€ run-producer.sh        # Lancer le producer
â”‚   â”œâ”€â”€ run-streaming.sh       # Lancer Spark Streaming
â”‚   â””â”€â”€ run-batch.sh           # Lancer analyses batch
â”‚
â”œâ”€â”€ sql/                        # RequÃªtes SQL
â”‚   â””â”€â”€ create-tables.sql      # CrÃ©ation tables Impala/Hive
â”‚
â”œâ”€â”€ notebooks/                  # Notebooks Jupyter
â”‚   â””â”€â”€ analysis.ipynb         # Analyses et visualisations
â”‚
â””â”€â”€ data/                       # DonnÃ©es de rÃ©fÃ©rence
    â””â”€â”€ ports.json             # CoordonnÃ©es des ports
```

---

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis

- Docker & Docker Compose installÃ©s
- 16 GB RAM minimum
- 20 GB espace disque
- SBT 1.9+ (pour compiler Scala)

### Ã‰tape 1 : Cloner et Configurer

```bash
# Cloner le projet
git clone <votre-repo>
cd maritime-tracking

# Rendre les scripts exÃ©cutables
chmod +x scripts/*.sh

# Lancer l'installation complÃ¨te
./scripts/setup-and-run.sh
```

**Ce script va :**
- âœ… DÃ©marrer tous les conteneurs Docker
- âœ… CrÃ©er les topics Kafka
- âœ… Initialiser les rÃ©pertoires HDFS
- âœ… Configurer SBT

### Ã‰tape 2 : Compiler le Code Scala

```bash
cd scala-app
sbt clean compile assembly
cd ..
```

Cela crÃ©e le JAR : `MaritimeTracking-assembly-1.0.jar`

### Ã‰tape 3 : Lancer le Pipeline

#### 3.1 DÃ©marrer le Producer Kafka

```bash
./scripts/run-producer.sh

# Avec paramÃ¨tres personnalisÃ©s
KAFKA_BROKERS=localhost:9092 NUM_SHIPS=10 INTERVAL=5 ./scripts/run-producer.sh
```

Vous devriez voir :
```
ğŸš¢ Flotte crÃ©Ã©e:
  â€¢ SHIP_001: Tanger â†’ Marseille (850 nm)
  â€¢ SHIP_002: Barcelona â†’ Alger (320 nm)
  ...

âœ“ Sent: SHIP_001 to partition 0 at offset 123
âš ï¸  ALERTE: SHIP_003 - Carburant bas (9500L)
```

#### 3.2 Lancer Spark Streaming

```bash
# Dans un nouveau terminal
./scripts/run-streaming.sh
```

AccÃ©dez Ã  l'UI Spark : http://localhost:4040

#### 3.3 Lancer l'Analyse Batch (aprÃ¨s ~5 min de donnÃ©es)

```bash
./scripts/run-batch.sh
```

---

## ğŸ“Š Analyses Disponibles

### 1. DonnÃ©es Temps RÃ©el (Spark Streaming)

- **AgrÃ©gations par fenÃªtre (5 min)** : Vitesse moyenne, consommation
- **DÃ©tection anomalies** : Carburant bas, vitesse anormale, problÃ¨me moteur
- **Calcul ETA** : Estimation temps d'arrivÃ©e

**Localisation HDFS** :
```
/maritime/raw_data/         # DonnÃ©es brutes partitionnÃ©es
/maritime/aggregated/       # AgrÃ©gations 5 min
/maritime/anomalies/        # Alertes dÃ©tectÃ©es
/maritime/eta_predictions/  # PrÃ©dictions ETA
```

### 2. Analyses Batch (Spark SQL)

| Analyse | Description | Fichier |
|---------|-------------|---------|
| **ship_statistics** | Stats par navire (vitesse, consommation, efficacitÃ©) | `/maritime/analysis/ship_statistics` |
| **route_performance** | Performance par route (temps, distance, conso) | `/maritime/analysis/route_performance` |
| **temporal_analysis** | Ã‰volution par heure de la journÃ©e | `/maritime/analysis/temporal_analysis` |
| **weather_impact** | Impact mÃ©tÃ©o sur performance | `/maritime/analysis/weather_impact` |
| **anomalies_detected** | Historique des anomalies | `/maritime/analysis/anomalies_detected` |
| **maintenance_prediction** | Score de risque maintenance | `/maritime/analysis/maintenance_prediction` |

---

## ğŸ—„ï¸ RequÃªtes SQL Impala/Hive

### CrÃ©er les Tables

```bash
# Copier le script SQL dans Hive
docker cp sql/create-tables.sql hive-metastore:/tmp/

# ExÃ©cuter
docker exec -it hive-metastore hive -f /tmp/create-tables.sql
```

### Exemples de RequÃªtes

```sql
-- Position actuelle de tous les navires
SELECT * FROM v_current_positions;

-- Navires nÃ©cessitant attention
SELECT * FROM v_ships_requiring_attention;

-- Top 10 routes les plus rapides
SELECT port_depart, port_arrivee, temps_estime_heures
FROM route_performance
ORDER BY temps_estime_heures ASC
LIMIT 10;

-- Navires en maintenance urgente
SELECT navire_id, score_risque, priorite_maintenance
FROM maintenance_prediction
WHERE priorite_maintenance = 'URGENT';

-- Distribution mÃ©tÃ©o
SELECT meteo, occurrences, 
       ROUND(100.0 * occurrences / SUM(occurrences) OVER(), 2) as pourcentage
FROM weather_impact;
```

---

## ğŸ“ˆ Visualisation avec Jupyter

### AccÃ©der Ã  Jupyter

1. Ouvrir : http://localhost:8888
2. Token : visible dans `docker logs jupyter`

### Exemple de Notebook

```python
from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
import pandas as pd

# CrÃ©er session Spark
spark = SparkSession.builder \
    .appName("MaritimeAnalysis") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .getOrCreate()

# Charger les statistiques des navires
df = spark.read.parquet("hdfs://namenode:9000/maritime/analysis/ship_statistics")

# Convertir en Pandas pour visualisation
pdf = df.toPandas()

# Graphique efficacitÃ© carburant
plt.figure(figsize=(12, 6))
plt.barh(pdf['navire_id'], pdf['efficacite_carburant_nm_per_litre'])
plt.xlabel('EfficacitÃ© (nm par litre)')
plt.ylabel('Navire')
plt.title('EfficacitÃ© Ã‰nergÃ©tique par Navire')
plt.tight_layout()
plt.show()

# Carte des trajectoires
import folium
from folium.plugins import HeatMap

# Charger positions
positions = spark.read.parquet("hdfs://namenode:9000/maritime/raw_data")
pos_pdf = positions.select("latitude", "longitude").toPandas()

# CrÃ©er carte centrÃ©e sur la MÃ©diterranÃ©e
m = folium.Map(location=[37.0, 3.0], zoom_start=5)

# Ajouter heatmap des trajectoires
HeatMap(pos_pdf[['latitude', 'longitude']].values.tolist()).add_to(m)

m.save('maritime_heatmap.html')
```

---

## ğŸ” Monitoring et Debugging

### Interfaces Web

| Service | URL | Description |
|---------|-----|-------------|
| Spark Master UI | http://localhost:8080 | Ã‰tat du cluster Spark |
| Spark Job UI | http://localhost:4040 | Jobs en cours d'exÃ©cution |
| HDFS NameNode | http://localhost:9870 | Exploration HDFS |
| Jupyter | http://localhost:8888 | Notebooks |

### Commandes Utiles

```bash
# Voir les logs d'un service
docker logs -f kafka
docker logs -f spark-master

# Ã‰tat des conteneurs
docker-compose ps

# RedÃ©marrer un service
docker-compose restart spark-master

# Entrer dans un conteneur
docker exec -it spark-master bash

# VÃ©rifier les topics Kafka
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Consommer un topic
docker exec kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic maritime-tracking \
    --from-beginning \
    --max-messages 10

# Explorer HDFS
docker exec namenode hdfs dfs -ls /maritime
docker exec namenode hdfs dfs -cat /maritime/raw_data/date=2025-12-22/*/part-00000*.parquet | head

# VÃ©rifier l'espace HDFS
docker exec namenode hdfs dfsadmin -report
```

---

## ğŸ“ Travail Ã  Rendre

### Rapport (10-20 pages)

#### Structure RecommandÃ©e

1. **Introduction**
   - Contexte du projet
   - Cas d'usage maritime
   - Objectifs

2. **Architecture**
   - SchÃ©ma du pipeline
   - Justification des choix technologiques
   - Flux de donnÃ©es

3. **ImplÃ©mentation**
   - **GÃ©nÃ©ration des donnÃ©es** (Scala)
     - ModÃ¨le de simulation
     - Formule Haversine pour calculs gÃ©ographiques
   - **Kafka** : Configuration, topics, producer
   - **Spark Streaming** : FenÃªtres de temps, watermarking
   - **HDFS** : Partitionnement, format Parquet
   - **Impala/Spark SQL** : Tables, vues, requÃªtes complexes

4. **Analyses et RÃ©sultats**
   - Statistiques descriptives
   - Visualisations (graphiques, cartes)
   - Insights mÃ©tier

5. **Performances**
   - Throughput Kafka
   - Latence de traitement Spark
   - ScalabilitÃ© du systÃ¨me

6. **Conclusion**
   - Apprentissages
   - Limites du projet
   - AmÃ©liorations futures

### Code Source

Structure Ã  inclure :
```
maritime-tracking-code/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ scala-app/
â”‚   â”œâ”€â”€ build.sbt
â”‚   â””â”€â”€ src/main/scala/maritime/
â”œâ”€â”€ scripts/
â”œâ”€â”€ sql/
â””â”€â”€ notebooks/
```

### Captures d'Ã‰cran

Ã€ inclure dans le rapport :
- âœ… Spark UI montrant les jobs en cours
- âœ… HDFS UI avec l'arborescence des fichiers
- âœ… Logs Kafka montrant les messages
- âœ… RÃ©sultats des requÃªtes SQL
- âœ… Graphiques de visualisation (Jupyter)
- âœ… Carte des trajectoires maritimes

---

## ğŸ“ Points ClÃ©s pour l'Ã‰valuation

### Spark (RDD, DataFrame, Spark SQL, Streaming)

âœ… **RDD** : Manipulation bas niveau dans `DataGenerator`  
âœ… **DataFrame** : Transformations dans `SparkStreaming` et `SparkBatch`  
âœ… **Spark SQL** : RequÃªtes complexes, agrÃ©gations, fenÃªtres  
âœ… **Spark Streaming** : Traitement temps rÃ©el avec watermarking

### Scala

âœ… **Classes case** : `Ship`, `ShipTelemetry`  
âœ… **Pattern matching** : Gestion des anomalies  
âœ… **Fonctions** : Haversine, calcul de cap  
âœ… **IntÃ©gration Spark** : Code idiomatique Scala

### Kafka

âœ… **Producer** : Envoi de messages JSON  
âœ… **Topics** : `maritime-tracking`, `maritime-alerts`  
âœ… **Consumer** : Spark Streaming lit depuis Kafka  
âœ… **Partitionnement** : Par `navire_id`

### Impala/Hive

âœ… **Tables externes** : Sur donnÃ©es Parquet HDFS  
âœ… **Partitionnement** : Par date et navire_id  
âœ… **Vues** : Positions actuelles, alertes, rÃ©sumÃ©s  
âœ… **RequÃªtes distribuÃ©es** : AgrÃ©gations complexes

### HDFS

âœ… **Stockage distribuÃ©** : RÃ©plication, tolÃ©rance aux pannes  
âœ… **Organisation** : HiÃ©rarchie `/maritime/raw_data/`, `/analysis/`  
âœ… **Format** : Parquet pour compression et performance

### Outil Additionnel : Docker

âœ… **Orchestration** : Docker Compose pour tous les services  
âœ… **Networking** : RÃ©seau `maritime-network`  
âœ… **Volumes** : Persistance des donnÃ©es

---

## ğŸ”§ Personnalisation et Extensions

### Ajouter Plus de Navires

```bash
NUM_SHIPS=20 ./scripts/run-producer.sh
```

### Modifier l'Intervalle de GÃ©nÃ©ration

```bash
INTERVAL=5 ./scripts/run-producer.sh  # Toutes les 5 secondes
```

### Ajouter de Nouveaux Ports

Ã‰diter `DataGenerator.scala` :

```scala
val ports = Map(
  "Tanger" -> (35.7595, -5.8340),
  "Marseille" -> (43.2965, 5.3698),
  "AthÃ¨nes" -> (37.9838, 23.7275),  // Nouveau
  "Istamboul" -> (41.0082, 28.9784) // Nouveau
)
```

### Ajouter de Nouvelles MÃ©triques

Dans `ShipTelemetry`, ajouter :

```scala
case class ShipTelemetry(
  // ... champs existants
  pression_atmospherique: Double,
  hauteur_vagues_metres: Double,
  direction_vent_degres: Double
)
```

---

## ğŸ› Troubleshooting

### ProblÃ¨me : Kafka ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker logs kafka

# RedÃ©marrer Zookeeper puis Kafka
docker-compose restart zookeeper
sleep 10
docker-compose restart kafka
```

### ProblÃ¨me : HDFS "Safe Mode"

```bash
docker exec namenode hdfs dfsadmin -safemode leave
```

### ProblÃ¨me : MÃ©moire insuffisante Spark

Dans `docker-compose.yml`, augmenter :

```yaml
spark-master:
  environment:
    - SPARK_DRIVER_MEMORY=4G  # Au lieu de 2G
```

### ProblÃ¨me : Compilation Scala Ã©choue

```bash
cd scala-app
rm -rf target project/target
sbt clean compile
```

---

## ğŸ“š Ressources ComplÃ©mentaires

### Documentation Officielle

- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Apache Kafka](https://kafka.apache.org/documentation/)
- [Apache Hadoop](https://hadoop.apache.org/docs/current/)
- [Scala](https://docs.scala-lang.org/)

### Concepts ClÃ©s

- **Streaming** : Traitement de flux de donnÃ©es en temps rÃ©el
- **Partitionnement** : Division des donnÃ©es pour parallÃ©lisme
- **Watermarking** : Gestion des donnÃ©es en retard (late data)
- **Windowing** : AgrÃ©gations sur fenÃªtres de temps

---

## ğŸ‘¥ Auteur

**Projet Big Data / Data Engineering**  
Module : Big Data Ecosystem  
Technologies : Kafka, Spark, Scala, HDFS, Impala

---

## ğŸ“„ Licence

Projet acadÃ©mique - Utilisation libre pour apprentissage.

---

## âœ… Checklist Finale

Avant de rendre votre projet, vÃ©rifiez :

- [ ] Tous les services Docker dÃ©marrent correctement
- [ ] Le producer Kafka gÃ©nÃ¨re des donnÃ©es
- [ ] Spark Streaming traite les flux en temps rÃ©el
- [ ] Les donnÃ©es sont Ã©crites dans HDFS (vÃ©rifier avec `hdfs dfs -ls`)
- [ ] L'analyse batch produit des rÃ©sultats
- [ ] Les tables Impala/Hive sont crÃ©Ã©es
- [ ] Les requÃªtes SQL s'exÃ©cutent sans erreur
- [ ] Le rapport PDF est complet (10-20 pages)
- [ ] Le code est commentÃ© et propre
- [ ] Les captures d'Ã©cran sont incluses
- [ ] Un README explique comment lancer le projet

---

**ğŸš¢ Bon courage pour votre projet Big Data !**